import torch
import torch.nn.functional as F
from tqdm.auto import tqdm
import hydra
from omegaconf import DictConfig, OmegaConf, SCMode

from diffusers import DDPMScheduler, UNet2DModel#, DDPMPipeline
from ai4ha.diffusion.pipelines.cond_pipeline_ddpm_labels import DDPMPipeline
from ai4ha.diffusion.models.unets.cond_unet_2d_labels import UNet2DConditionModel
from VQ-VAE.VQVAEModel_positional import positional_VQVAE, get_time_embedding
from dataloader import *


from accelerate import Accelerator
from accelerate.logging import get_logger
import logging
import os

import torchvision.transforms as T
from torchvision.utils import make_grid

from accelerate import DistributedDataParallelKwargs
import math
from time import time
import numpy as np

from ai4ha.log.textlog import textlog
from ai4ha.Autoencoders import AutoencoderKL, VQModel


from ai4ha.util import (
    instantiate_from_config,
    time_management,
    fix_paths,
    experiment_name_autoencoder,
    save_config,
)
from ai4ha.util.train import (
    get_most_recent_checkpoint,
    save_checkpoint_accelerate,
    get_optimizer,
    get_lr_scheduler,
)

import accelerate
import timm
import torchvision
from accelerate.utils import DistributedType, ProjectConfiguration, set_seed
from packaging import version
from timm.data import resolve_data_config
from timm.data.transforms_factory import create_transform
from torchvision import transforms
from tqdm.auto import trange
import matplotlib.pyplot as plt
from diffusers.optimization import get_cosine_schedule_with_warmup

DIRS = ["checkpoints", "logs", "samples", "final", "model"]
logger = get_logger(__name__, log_level="INFO")


def visualize_reconstructions(real_samples, reconstructed_samples, iteration, BASE_DIR, num_samples=4):
    """
        Visulaize reconstructions of the VQ-VAE.
    """
    # Choose a subset of samples to visualize
    real_samples = real_samples[:num_samples].detach().cpu()
    reconstructed_samples = reconstructed_samples[:num_samples].detach().cpu()
    path = f"{BASE_DIR}samples/E{iteration:04d}-S{iteration + 1:05d}s.png"
    # Plot the real and reconstructed samples
    fig, axes = plt.subplots(2, num_samples, figsize=(15, 4))

    for i in range(num_samples):
        # Real samples (top row)
        axes[0, i].imshow(real_samples[i].permute(1, 2, 0),
                          cmap=plt.get_cmap('gray'))  # If image data, permute for channels last
        axes[0, i].axis('off')
        axes[0, i].set_title("Real")

        # Reconstructed samples (bottom row)
        axes[1, i].imshow(reconstructed_samples[i].permute(1, 2, 0), cmap=plt.get_cmap('gray'))  # Same here
        axes[1, i].axis('off')
        axes[1, i].set_title("Reconstructed")

    fig.savefig(path)


def extract_latents_vqvae(images, vqvae, pos_emb):
    """
        Extracts latent representations from input images using a VQ-VAE (Vector Quantized Variational Autoencoder) model.
    """
    with torch.no_grad():
        if hasattr(vqvae, 'module'):
            latents = vqvae.module.encode(images, pos_emb)
        else:
            latents = vqvae.encode(images, pos_emb)
        batch_size, channels, height, width = latents.shape
        x = latents.permute(0, 2, 3, 1).contiguous()  # Change to [batch_size, height, width, channels]
        x = x.view(batch_size * height * width, channels)  # Flatten to [batch_size * height * width, channels]
        if hasattr(vqvae, 'module'):
            x, indices, commit_loss = vqvae.module.quantizer(x)
        else:
            x, indices, commit_loss = vqvae.quantizer(x)
        latents = x.view(batch_size, height, width, channels).permute(0, 3, 1, 2).contiguous()
    return latents

def latents_std_vqvae(vqvae, train_dataloader, accelerator, preloop=50):
    vqvae.scaling_factor = 1.0
    # Preloop measuring of stdev of the latent
    latent_std = 0
    for step, batch in enumerate(train_dataloader):
        clean_images = batch['image'].permute(0, 3, 1, 2).to(memory_format=torch.contiguous_format).to(
            accelerator.device)
        positions = batch["position"].to(
            memory_format=torch.contiguous_format).to(accelerator.device)
        pos_emb = get_time_embedding(positions, temb_dim=190)

        clean_images_latents = extract_latents_vqvae(clean_images, vqvae.to(accelerator.device), pos_emb)
        latent_std += clean_images_latents.detach().flatten().std()
        if step > preloop:
            break

    vqvae.scaling_factor = 1.0 / (latent_std / preloop)
    print(1.0 / (latent_std / preloop))


def evaluate(iteration, ddpm_pipeline, vqvae, BASE_DIR, sample_size=16, accelerator=None):
    """
            Visulaize a MRI volume every 10 positions generated by the LDM model.
    """

    path = f"{BASE_DIR}samples/2Diff-E{iteration:04d}-S{iteration + 1:05d}s.png"
    device = accelerator.device if accelerator else "cuda"
    decoded_images = []
    seeds = 508
    positions = [3, 13, 23, 33, 43, 53, 63, 73, 83, 93, 103, 113, 123, 133, 143, 153]
    for i in range(sample_size):
        pos_emb = get_time_embedding(torch.tensor([positions[i]]), temb_dim=190)
        generated_latent = ddpm_pipeline(batch_size=1, generator=torch.Generator(device='cpu').manual_seed(seeds),
                                         output_type='np.array', num_inference_steps=1000, latent=True, vae=vqvae,
                                         position=pos_emb.to(device), class_cond=(torch.tensor([positions[i]]).to(device), torch.tensor([1]).to(device)), accelerator=accelerator).images
        decoded_images.append(generated_latent[0])

    decoded_images = torch.stack(decoded_images, axis=0)

    # Display and save the generated samples
    print('Samples')
    print(torch.max(decoded_images))
    print(torch.min(decoded_images))
    print(decoded_images.shape)
    grid = torchvision.utils.make_grid(decoded_images, nrow=4, normalize=True)

    plt.figure(figsize=(8, 8))
    plt.imshow(grid.permute(1, 2, 0).cpu().numpy())
    plt.title(f"Generated Samples at Epoch {iteration}")
    plt.axis("off")
    plt.savefig(path)
    plt.close()


def train(model, train_loader, image_key, BASE_DIR, num_epochs=10, alpha=10, device='cuda', opt=None, num_codes=256,
          visualize_freq=10):
    # Initialize the Accelerator
    accelerator = Accelerator()
    device = accelerator.device

    # Prepare the model, optimizer, and dataloader with accelerator
    model, opt, train_loader = accelerator.prepare(model, opt, train_loader)

    # Progress bar for tracking iterations
    best_loss = 1000
    for epoch in range(num_epochs):
        progress_bar = tqdm(total=len(train_loader),
                            disable=not accelerator.is_local_main_process)
        training_data = {
            "rec_loss": [],
            "cmt_loss": []
        }
        for i, batch in enumerate(train_loader):

            pixel_values = (batch[image_key].permute(0, 3, 1, 2).to(
                memory_format=torch.contiguous_format).to(accelerator.device))
            opt.zero_grad()

            out, indices, cmt_loss = model(pixel_values)  # Forward pass through the model
            rec_loss = (out - pixel_values).abs().mean()  # Reconstruction loss

            cmt_loss_scalar = cmt_loss.mean()  # Handle multi-element cmt_loss

            # Backpropagation (wrapped by accelerator)
            accelerator.backward(rec_loss + alpha * cmt_loss_scalar)

            opt.step()  # Update model parameters

            progress_bar.set_description(
                f"Epoch {epoch} | " + f"rec loss: {rec_loss.item():.3f} | "
                + f"cmt loss: {cmt_loss_scalar.item():.3f} | "
                + f"active %: {indices.unique().numel() / num_codes * 100:.3f}"
            )
            training_data['rec_loss'].append(rec_loss.item())
            training_data['cmt_loss'].append(cmt_loss_scalar.item())

        print(f"Epoch: {epoch} | Reconstruction loss: {np.mean(training_data['rec_loss']):.3f} |  Cmt loss: {np.mean(training_data['cmt_loss']):.3f}")

        if epoch % visualize_freq == 0:
            real_samples, reconstructed_samples = accelerator.gather(pixel_values), accelerator.gather(out)
            visualize_reconstructions(real_samples, reconstructed_samples, epoch, BASE_DIR)

        if np.mean(training_data['rec_loss']) < best_loss:
            best_loss = np.mean(training_data['rec_loss'])
            torch.save(model, f'{BASE_DIR}model/VQVAE-best-model-epoch{epoch}.pt')
            torch.save(model.state_dict(), f'{BASE_DIR}model/VQVAE-best-model-parameters-epoch{epoch}.pt')
            best_model_data = {
                "Epoch": epoch,
                "Reconstruction loss": np.mean(training_data['rec_loss']),
                "Cmt loss": np.mean(training_data['cmt_loss'])
            }

    print("End of Training")
    print(f"BEST MODEL INFO  |  Reconstruction loss: {best_model_data['Reconstruction loss']:.3f}  |  Cmt loss: {best_model_data['Cmt loss']:.3f}  |  Epoch: {best_model_data['Epoch']}")



def train_diffusion(vqvae, scheduler, unet, train_dataloader, optimizer,  BASE_DIR, lr_scheduler, epochs = 50, compute_latents_std=True, visualize_freq=5):
    vqvae.eval()
    accelerator = Accelerator()
    unet, optimizer, train_loader, scheduler, lr_scheduler, vqvae = accelerator.prepare(unet, optimizer, train_dataloader, scheduler, lr_scheduler, vqvae)

    if compute_latents_std:
        latents_std_vqvae(vqvae, train_dataloader, accelerator)
    best_loss = 1000
    for epoch in range(epochs):
        training_data = {
            "loss": []
        }
        for batch in train_loader:
            images = (batch["image"].permute(0, 3, 1, 2).to(
                memory_format=torch.contiguous_format).to(accelerator.device))

            positions = (batch["position"].to(
                memory_format=torch.contiguous_format).to(accelerator.device))

            label = (batch["label"].to(
                memory_format=torch.contiguous_format).to(accelerator.device))

            pos_emb = get_time_embedding(positions, temb_dim=190)
            latents = extract_latents_vqvae(images, vqvae.to(accelerator.device), pos_emb)
            noise = torch.randn_like(latents)
            timesteps = torch.randint(0, scheduler.num_train_timesteps, (latents.shape[0],)).long()
            noised_latents = scheduler.add_noise(latents, noise, timesteps)
            with accelerator.accumulate(unet):
                pred_noise = unet(noised_latents, timesteps.to(accelerator.device), class_labels=positions,encoder_hidden_states=None, added_cond_kwargs={'labels':label.unsqueeze(1)}).sample

                loss = torch.nn.functional.mse_loss(pred_noise, noise)

                accelerator.backward(loss)
                lr_scheduler.step()
                optimizer.step()
                optimizer.zero_grad()
                training_data['loss'].append(loss.item())

        if accelerator.is_main_process:
            print(f"Epoch: {epoch} | Loss: {np.mean(training_data['loss']):.5f}")

            ddpm_pipeline = DDPMPipeline(unet=accelerator.unwrap_model(unet), scheduler=scheduler)

            if epoch % visualize_freq == 0:
                evaluate(epoch + 1, ddpm_pipeline, vqvae, BASE_DIR, sample_size=16, accelerator=accelerator)
                
            if np.mean(training_data['loss']) < best_loss:
                best_loss = np.mean(training_data['loss'])
                ddpm_pipeline.save_pretrained(f'{BASE_DIR}model/DDPMpipeline-best-model')
                best_model_data = {
                    "Epoch": epoch,
                    "Loss": np.mean(training_data['loss'])
                }
                print(f"Saved best model at epoch: {epoch}")


def main(config):
    BASE_DIR = f"{config['base_dir_logs']}{config['name']}/"
    print(BASE_DIR)
    os.makedirs(f"{BASE_DIR}", exist_ok=True)
    
    DIRS = ["checkpoints", "logs", "samples", "final", "model"]
    for dir in DIRS:
        os.makedirs(f"{BASE_DIR}/{dir}", exist_ok=True)

    image_key = ("image" if "image_key" not in config["model"] else
                 config["model"]["image_key"])

    train_data = NautilusMRIPatientSliceDataset(config["model"]["data"])
    train_dataloader = torch.utils.data.DataLoader(train_data, **config["dataloader"])


    if config["pretrainedVQ"]:
        vqvae = positional_VQVAE(**config["model"]["vae"])
        if config["pretrainedVQ"]:
            vqvae.load_state_dict(torch.load(config["model"]["pretraiendVQ_path"], weights_only=True))
    else:
        opt = torch.optim.AdamW(vqvae.parameters(), lr=1e-4)
        vqvae = train(vqvae, train_dataloader, image_key, BASE_DIR, num_epochs=config["train"]["num_epochs"], opt=opt)


    scheduler = DDPMScheduler(
        **config["model"]["scheduler"]
    )

    latent_channels = config["model"]["unet"]["in_channels"]
    image_size = config["model"]["unet"]["sample_size"]

    unet = UNet2DConditionModel(
        class_embed_type = "timestep",
        encoder_hid_dim_type="labels",
        encoder_hid_dim=1024,
        **config["model"]["unet"]
    )

    optimizer = torch.optim.Adam(unet.parameters(), lr=1e-4)
    lr_scheduler = get_cosine_schedule_with_warmup(
        optimizer=optimizer,
        num_warmup_steps=500,
        num_training_steps=(len(train_dataloader) * 25),
    )
    train_diffusion(vqvae, scheduler, unet, train_dataloader, optimizer, BASE_DIR, lr_scheduler)


@hydra.main(version_base=None, config_path="conf", config_name="config")
def LatentDDPM(cfg: DictConfig) -> None:
    # Convert to dictionary so we can modify it
    cfg = OmegaConf.to_container(cfg, structured_config_mode=SCMode.DICT_CONFIG)

    cfg = fix_paths(cfg, cfg["local"])
    cfg["name"] = cfg["model"]["modelname"]
    main(cfg)


if __name__ == "__main__":
    LatentDDPM()